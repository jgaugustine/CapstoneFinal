{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a87af38",
   "metadata": {},
   "source": [
    "This explains how I implemented the image transformations in `ImageLab/src/components/ImageCanvas.tsx`. I present each transform as an algorithm, prove its correctness, and explain my design decisions.\n",
    "\n",
    "## 1. Color Space and Design\n",
    "\n",
    "### 1.1 sRGB\n",
    "\n",
    "All modern displays, web browsers, and the HTML Canvas API work in sRGB color space. sRGB values are gamma-encoded. They don't represent linear light intensity, but rather perceptually-uniform brightness levels optimized for human vision.\n",
    "\n",
    "This means that...\n",
    "- Values range from 0 to 255 for each channel (R, G, B)\n",
    "- The relationship to linear light is non-linear: $R_{linear} \\approx (R_{sRGB}/255)^{2.2}$\n",
    "- Value 128 appears as perceptual \"middle gray\" on screen (not 50% light intensity)\n",
    "- Gamma encoding makes efficient use of 8 bits by allocating more values to darker tones where human eyes are more sensitive\n",
    "\n",
    "### 1.2 Design Decisions\n",
    "\n",
    "I chose to perform all transformations in sRGB space (without linearization because):\n",
    "\n",
    "1. Linearizing and re-encoding requires power operations per pixel which are expensive and computation matters as we are working with large images\n",
    "2. For real-time image editing, gamma-space operations provide visually acceptable results\n",
    "3. Canvas ImageData provides sRGB values\n",
    "\n",
    "This means most transforms work on gamma-encoded values. I've ensured each algorithm is mathematically sound in this space.\n",
    "\n",
    "As I am most comfortable with Python, I originally thought it would be most computationally efficient to leverage libraries for the matrix operations. However, when looking through the documentation for libraries like ml-matrix or numeric.js, I noticed they require individual function calls for each operation. Unlike NumPy, which delegates to optimized C/Fortran code and processes batched operations in compiled code, JavaScript matrix libraries execute in the JavaScript runtime. For our use case—applying a 3×3 transformation to 2+ million pixels—this means 2+ million separate function calls, each with JavaScript overhead, object allocations, and garbage collection pressure.\n",
    "\n",
    "Therefore, I write all matrix operations inline by extracting matrix elements and computing per pixel.\n",
    "\n",
    "### 1.3 Mathematical Notation\n",
    "\n",
    "- $\\mathbf{r} = [R, G, B]^T$ represents a pixel in gamma-encoded sRGB space, where each component is in $[0, 255]$\n",
    "- $I$ denotes the $3 \\times 3$ identity matrix\n",
    "- $M$ denotes a $3 \\times 3$ transformation matrix\n",
    "- $\\mathbf{o}$ denotes a $3 \\times 1$ offset vector\n",
    "- An affine transform has the form: $\\mathbf{r}' = M\\mathbf{r} + \\mathbf{o}$\n",
    "- All operations include implicit clamping to $[0, 255]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48b9b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Brightness Adjustment\n",
    "\n",
    "**What I wanted:** A transform that shifts all color channels by the same amount, making the image uniformly lighter or darker.\n",
    "\n",
    "### Algorithm 2.1: Brightness Transform\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, brightness offset b ∈ ℝ\n",
    "Output: transformed pixel r' = [R', G', B']ᵀ\n",
    "\n",
    "1. R' ← clamp(R + b, 0, 255)\n",
    "2. G' ← clamp(G + b, 0, 255)  \n",
    "3. B' ← clamp(B + b, 0, 255)\n",
    "4. return [R', G', B']ᵀ\n",
    "```\n",
    "\n",
    "**Affine form:** $\\mathbf{r}' = I\\mathbf{r} + b\\mathbf{1}$ where $\\mathbf{1} = [1, 1, 1]^T$\n",
    "\n",
    "**Implementation:** See `buildBrightnessMatrix()` at lines 40-49 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const buildBrightnessMatrix = (value: number): { matrix: number[]; offset: number[] } => {\n",
    "  const matrix = [1, 0, 0, 0, 1, 0, 0, 0, 1]; // Identity matrix\n",
    "  const offset = [value, value, value]; // Uniform offset\n",
    "  return { matrix, offset };\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Correctness\n",
    "\n",
    "**Claim:** This transform shifts all channels uniformly while preserving color relationships.\n",
    "\n",
    "**Proof:**\n",
    "For any two pixels $\\mathbf{r}_1, \\mathbf{r}_2$:\n",
    "$$\\mathbf{r}'_1 - \\mathbf{r}'_2 = (I\\mathbf{r}_1 + b\\mathbf{1}) - (I\\mathbf{r}_2 + b\\mathbf{1}) = I(\\mathbf{r}_1 - \\mathbf{r}_2) = \\mathbf{r}_1 - \\mathbf{r}_2$$\n",
    "\n",
    "The difference between any two pixels remains constant, preserving relative color relationships. The transform is a translation in RGB space. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e75e26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Contrast Adjustment\n",
    "\n",
    "**What I wanted:** A transform that scales distances from middle gray (128), making dark regions darker and bright regions brighter while keeping middle gray fixed.\n",
    "\n",
    "### Algorithm 3.1: Contrast Transform\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, contrast factor c ∈ ℝ⁺\n",
    "Output: transformed pixel r'\n",
    "\n",
    "1. For each channel k ∈ {R, G, B}:\n",
    "2.   k' ← 128 + c(k - 128)\n",
    "3.   k' ← clamp(k', 0, 255)\n",
    "4. return r'\n",
    "```\n",
    "\n",
    "**Affine form:** $\\mathbf{r}' = cI\\mathbf{r} + 128(1-c)\\mathbf{1}$\n",
    "\n",
    "**Implementation:** See `buildContrastMatrix()` at lines 53-62 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const buildContrastMatrix = (value: number): { matrix: number[]; offset: number[] } => {\n",
    "  const matrix = [value, 0, 0, 0, value, 0, 0, 0, value]; // Scale by c\n",
    "  const offset = [128 * (1 - value), 128 * (1 - value), 128 * (1 - value)];\n",
    "  return { matrix, offset };\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Correctness\n",
    "\n",
    "**Claim:** Middle gray (128) is a fixed point, and distances from middle gray scale by factor $c$.\n",
    "\n",
    "**Proof:**\n",
    "Let $\\mathbf{g} = [128, 128, 128]^T$ represent middle gray in sRGB.\n",
    "\n",
    "**(1) Fixed point property:**\n",
    "$$\\mathbf{g}' = cI\\mathbf{g} + 128(1-c)\\mathbf{1} = c \\cdot 128 + 128(1-c) = 128 = \\mathbf{g}$$\n",
    "\n",
    "**(2) Distance scaling:**\n",
    "For any pixel $\\mathbf{r}$, the distance from middle gray after transformation is:\n",
    "$$\\mathbf{r}' - \\mathbf{g} = (cI\\mathbf{r} + 128(1-c)\\mathbf{1}) - 128\\mathbf{1} = c(\\mathbf{r} - 128\\mathbf{1}) = c(\\mathbf{r} - \\mathbf{g})$$\n",
    "\n",
    "Thus distances from middle gray scale by exactly $c$. \n",
    "\n",
    "*Note:* In sRGB, the value 128 represents perceptual middle gray (approximately 50% brightness as perceived by human vision), making it the natural anchor point for contrast adjustments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95f3d6",
   "metadata": {},
   "source": [
    "## 4. Saturation Adjustment\n",
    "\n",
    "**What I wanted:** A transform that blends each color with its grayscale equivalent, controlling color intensity while preserving perceived brightness.\n",
    "\n",
    "### Algorithm 4.1: Saturation Transform\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, saturation factor s ∈ ℝ\n",
    "Output: transformed pixel r'\n",
    "\n",
    "1. L ← 0.299·R + 0.587·G + 0.114·B    // Gamma-space luminance\n",
    "2. gray ← [L, L, L]ᵀ\n",
    "3. r' ← gray·(1 - s) + r·s             // Linear interpolation\n",
    "4. return clamp(r', 0, 255)\n",
    "```\n",
    "\n",
    "**Affine form:** This can be expressed as $\\mathbf{r}' = M_s\\mathbf{r}$ where:\n",
    "\n",
    "$$M_s = \\begin{bmatrix}\n",
    "w_R + (1-w_R)s & w_G(1-s) & w_B(1-s) \\\\\n",
    "w_R(1-s) & w_G + (1-w_G)s & w_B(1-s) \\\\\n",
    "w_R(1-s) & w_G(1-s) & w_B + (1-w_B)s\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $(w_R, w_G, w_B) = (0.299, 0.587, 0.114)$ are the **Rec.601 luminance weights**.\n",
    "\n",
    "**Implementation:** See `buildSaturationMatrixGamma()` at lines 68-95 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const buildSaturationMatrixGamma = (saturation: number): number[] => {\n",
    "  if (saturation === 1) return [1, 0, 0, 0, 1, 0, 0, 0, 1];\n",
    "  \n",
    "  const wR = 0.299, wG = 0.587, wB = 0.114;\n",
    "  const s = saturation;\n",
    "  \n",
    "  return [\n",
    "    wR + (1 - wR) * s, wG * (1 - s), wB * (1 - s),\n",
    "    wR * (1 - s), wG + (1 - wG) * s, wB * (1 - s),\n",
    "    wR * (1 - s), wG * (1 - s), wB + (1 - wB) * s\n",
    "  ];\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Correctness\n",
    "\n",
    "**Claim:** The saturation transform (1) preserves gamma-space luminance and (2) leaves gray pixels invariant.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**(1) Gamma-space luminance preservation:**\n",
    "\n",
    "The gamma-space luminance of the transformed pixel is:\n",
    "$$L' = w_R R' + w_G G' + w_B B'$$\n",
    "\n",
    "Substituting $\\mathbf{r}' = L\\mathbf{1}(1-s) + \\mathbf{r} \\cdot s$:\n",
    "$$L' = w_R[L(1-s) + Rs] + w_G[L(1-s) + Gs] + w_B[L(1-s) + Bs]$$\n",
    "$$= L(1-s)(w_R + w_G + w_B) + s(w_R R + w_G G + w_B B)$$\n",
    "$$= L(1-s) \\cdot 1 + s \\cdot L = L$$\n",
    "\n",
    "The gamma-space luminance is preserved. \n",
    "\n",
    "**(2) Gray invariance:**\n",
    "\n",
    "Let $\\mathbf{r} = [g, g, g]^T$ be a gray pixel. Then:\n",
    "$$L = w_R g + w_G g + w_B g = g(w_R + w_G + w_B) = g$$\n",
    "\n",
    "Thus:\n",
    "$$\\mathbf{r}' = [g, g, g](1-s) + [g, g, g]s = [g, g, g]$$\n",
    "\n",
    "Gray pixels remain unchanged for all $s$. \n",
    "\n",
    "\n",
    "I use Rec.601 weights $(0.299, 0.587, 0.114)$ rather than the more modern Rec.709 weights $(0.2126, 0.7152, 0.0722)$ because Rec.709 weights are designed for linear light. Rec.601 weights are empirically better for gamma space. \n",
    "\n",
    "However, there is no perfect set of constant weights for gamma-space luminance because gamma encoding is non-linear. The most correct approach would be to linearize -> apply Rec.709 -> apply saturation -> transform to sRGB. However, this requires expensive power operations and the Rec.601 approximation provides acceptable perceptual results for real-time editing while maintaining our performance advantage of working natively in sRGB. \n",
    "\n",
    "*Note:* I want to add a feature later that allows use to compare the edit applied in linear- and gamma- encoded light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297196fa",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Vibrance Adjustment\n",
    "\n",
    "**What I wanted:** An adaptive saturation that boosts dull colors more than already-saturated colors, preventing oversaturation of skin tones and vivid regions.\n",
    "\n",
    "### Algorithm 5.1: Vibrance Transform\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, vibrance factor v ∈ ℝ\n",
    "Output: transformed pixel r'\n",
    "\n",
    "1. maxC ← max(R, G, B)\n",
    "2. minC ← min(R, G, B)\n",
    "3. ŝ ← (maxC - minC) / maxC        // Saturation estimate\n",
    "4. f ← 1 + v(1 - ŝ)                 // Adaptive factor\n",
    "5. L ← 0.299·R + 0.587·G + 0.114·B\n",
    "6. r' ← [L, L, L]ᵀ + f(r - [L, L, L]ᵀ)\n",
    "7. return clamp(r', 0, 255)\n",
    "```\n",
    "\n",
    "**Key insight:** Unlike saturation, the factor $f$ depends on each pixel's current saturation $\\hat{s}(\\mathbf{r})$, making this inherently per-pixel.\n",
    "\n",
    "**Implementation:** See `applyVibranceGamma()` at lines 194-210 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const applyVibranceGamma = (rgb: RGB, vibrance: number): RGB => {\n",
    "  if (vibrance === 0) return rgb;\n",
    "  const R = rgb.r, G = rgb.g, B = rgb.b;\n",
    "  const maxC = Math.max(R, G, B), minC = Math.min(R, G, B);\n",
    "  const sEst = maxC === 0 ? 0 : (maxC - minC) / maxC;\n",
    "  const f = 1 + vibrance * (1 - sEst);\n",
    "  const gray = 0.299 * R + 0.587 * G + 0.114 * B;\n",
    "  if (R === G && G === B) return { r: R, g: G, b: B };\n",
    "  return { \n",
    "    r: clamp(gray + (R - gray) * f), \n",
    "    g: clamp(gray + (G - gray) * f), \n",
    "    b: clamp(gray + (B - gray) * f) \n",
    "  };\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Non-Composability\n",
    "\n",
    "Vibrance cannot be expressed as a global affine transform (matrix + offset).\n",
    "\n",
    "**Proof by contradiction:**\n",
    "\n",
    "Assume vibrance can be expressed as $V(\\mathbf{r}) = M\\mathbf{r} + \\mathbf{o}$ for some fixed $M$ and $\\mathbf{o}$.\n",
    "\n",
    "Consider two pixels:\n",
    "- $\\mathbf{r}_1 = [255, 0, 0]^T$ (saturated red): $\\hat{s}_1 = 1$, so $f_1 = 1 + v(1-1) = 1$\n",
    "- $\\mathbf{r}_2 = [200, 195, 190]^T$ (desaturated): $\\hat{s}_2 \\approx 0.05$, so $f_2 = 1 + v(0.95) \\approx 1 + 0.95v$\n",
    "\n",
    "For $v = 1$, we have $f_1 = 1$ and $f_2 \\approx 1.95$.\n",
    "\n",
    "If vibrance were a linear transform, the ratio of how much each pixel changes would be constant:\n",
    "$$\\frac{\\|\\mathbf{r}'_1 - L_1\\mathbf{1}\\|}{\\|\\mathbf{r}_1 - L_1\\mathbf{1}\\|} = \\frac{\\|\\mathbf{r}'_2 - L_2\\mathbf{1}\\|}{\\|\\mathbf{r}_2 - L_2\\mathbf{1}\\|}$$\n",
    "\n",
    "But by construction:\n",
    "$$\\frac{\\|\\mathbf{r}'_1 - L_1\\mathbf{1}\\|}{\\|\\mathbf{r}_1 - L_1\\mathbf{1}\\|} = f_1 = 1 \\neq 1.95 \\approx f_2 = \\frac{\\|\\mathbf{r}'_2 - L_2\\mathbf{1}\\|}{\\|\\mathbf{r}_2 - L_2\\mathbf{1}\\|}$$\n",
    "\n",
    "This contradicts linearity. Therefore, vibrance cannot be a global affine transform. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de48d82",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Hue Rotation\n",
    "\n",
    "**What I wanted:** A transform that rotates colors around the gray axis in RGB space, changing hue while preserving brightness and leaving grays unchanged.\n",
    "\n",
    "### Algorithm 6.1: Hue Rotation\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, angle θ (in degrees)\n",
    "Output: rotated pixel r'\n",
    "\n",
    "1. Convert θ to radians: θ_rad ← θ · π/180\n",
    "2. Compute rotation matrix R(θ) using Rodrigues' formula\n",
    "3. r' ← R(θ) · r\n",
    "4. return clamp(r', 0, 255)\n",
    "```\n",
    "\n",
    "**Mathematical form:** I use Rodrigues' rotation formula for rotation by angle $\\theta$ around the normalized gray axis $\\mathbf{u} = \\frac{1}{\\sqrt{3}}[1, 1, 1]^T$:\n",
    "\n",
    "$$R(\\theta) = \\cos\\theta \\cdot I + (1-\\cos\\theta)(\\mathbf{u}\\mathbf{u}^T) + \\sin\\theta \\cdot [\\mathbf{u}]_\\times$$\n",
    "\n",
    "where $[\\mathbf{u}]_\\times$ is the skew-symmetric cross-product matrix:\n",
    "\n",
    "$$[\\mathbf{u}]_\\times = \\frac{1}{\\sqrt{3}}\\begin{bmatrix}0 & -1 & 1\\\\1 & 0 & -1\\\\-1 & 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "After algebraic simplification, this becomes:\n",
    "\n",
    "$$R(\\theta) = \\begin{bmatrix}\n",
    "c + \\frac{1-c}{3} & \\frac{1-c}{3} - \\frac{s}{\\sqrt{3}} & \\frac{1-c}{3} + \\frac{s}{\\sqrt{3}} \\\\\n",
    "\\frac{1-c}{3} + \\frac{s}{\\sqrt{3}} & c + \\frac{1-c}{3} & \\frac{1-c}{3} - \\frac{s}{\\sqrt{3}} \\\\\n",
    "\\frac{1-c}{3} - \\frac{s}{\\sqrt{3}} & \\frac{1-c}{3} + \\frac{s}{\\sqrt{3}} & c + \\frac{1-c}{3}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $c = \\cos\\theta$ and $s = \\sin\\theta$.\n",
    "\n",
    "**Implementation:** See `buildHueMatrix()` at lines 98-123 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const buildHueMatrix = (value: number): number[] => {\n",
    "  if (value === 0) return [1, 0, 0, 0, 1, 0, 0, 0, 1];\n",
    "  \n",
    "  const angle = (value * Math.PI) / 180;\n",
    "  const cosA = Math.cos(angle);\n",
    "  const sinA = Math.sin(angle);\n",
    "  \n",
    "  return [\n",
    "    cosA + (1 - cosA) / 3,\n",
    "    1/3 * (1 - cosA) - Math.sqrt(1/3) * sinA,\n",
    "    1/3 * (1 - cosA) + Math.sqrt(1/3) * sinA,\n",
    "    1/3 * (1 - cosA) + Math.sqrt(1/3) * sinA,\n",
    "    cosA + 1/3 * (1 - cosA),\n",
    "    1/3 * (1 - cosA) - Math.sqrt(1/3) * sinA,\n",
    "    1/3 * (1 - cosA) - Math.sqrt(1/3) * sinA,\n",
    "    1/3 * (1 - cosA) + Math.sqrt(1/3) * sinA,\n",
    "    cosA + 1/3 * (1 - cosA)\n",
    "  ];\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Properties\n",
    "\n",
    "**Claim 1:** Any gray pixel $\\mathbf{g} = [g, g, g]^T$ remains unchanged under hue rotation.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "By definition of rotation, any vector parallel to the rotation axis is unchanged. Since $\\mathbf{g} = g[1,1,1]^T = g\\sqrt{3} \\cdot \\mathbf{u}$ is parallel to $\\mathbf{u}$:\n",
    "\n",
    "$$R(\\theta)\\mathbf{g} = R(\\theta)(g\\sqrt{3}\\mathbf{u}) = g\\sqrt{3} \\cdot R(\\theta)\\mathbf{u} = g\\sqrt{3} \\cdot \\mathbf{u} = \\mathbf{g}$$\n",
    "\n",
    "The last equality follows because rotating a vector around itself leaves it unchanged. ∎\n",
    "\n",
    "**Claim 2:** $R(\\theta)$ is an orthogonal matrix, preserving distances and angles.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Rodrigues' formula constructs rotation matrices by definition. All rotation matrices satisfy the orthogonality property $R(\\theta)^T R(\\theta) = I$.\n",
    "\n",
    "This can be verified directly from the formula: the three components $\\cos\\theta \\cdot I$, $(1-\\cos\\theta)(\\mathbf{u}\\mathbf{u}^T)$, and $\\sin\\theta \\cdot [\\mathbf{u}]_\\times$ are constructed such that their combination produces an orthogonal matrix.\n",
    "\n",
    "As a consequence, for any two pixels $\\mathbf{r}_1, \\mathbf{r}_2$:\n",
    "$$\\|\\mathbf{r}'_1 - \\mathbf{r}'_2\\| = \\|R(\\mathbf{r}_1 - \\mathbf{r}_2)\\| = \\|\\mathbf{r}_1 - \\mathbf{r}_2\\|$$\n",
    "\n",
    "Hue rotation preserves all distances in RGB space. ∎\n",
    "\n",
    "*Note:* Hue rotation is a geometric operation (it works identically in linear or gamma) encoded RGB space since it's just a 3D rotation. The operation is space-independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd238b8",
   "metadata": {},
   "source": [
    "## 7. Whites and Blacks Adjustments\n",
    "\n",
    "**What I wanted:** Two tone curve adjustments that target specific tonal ranges—whites for bright tones and blacks for dark tones—with smooth parametric transitions. These adjustments allow precise control over highlights and shadows while preserving detail in other tonal ranges, similar to Lightroom's Whites and Blacks sliders.\n",
    "\n",
    "### Algorithm 7.1: Smoothstep Function\n",
    "\n",
    "The smoothstep function provides a smooth S-curve transition between two edge values:\n",
    "\n",
    "```\n",
    "Input: edge values edge₀, edge₁ ∈ ℝ, input x ∈ ℝ\n",
    "Output: smoothstep weight w ∈ [0, 1]\n",
    "\n",
    "1. t ← clamp((x - edge₀)/(edge₁ - edge₀), 0, 1)\n",
    "2. w ← t² × (3 - 2t)\n",
    "3. return w\n",
    "```\n",
    "\n",
    "**Mathematical form:** \n",
    "\n",
    "$$\\text{smoothstep}(x) = t^2(3 - 2t), \\quad \\text{where } t = \\text{clamp}\\left(\\frac{x - \\text{edge}_0}{\\text{edge}_1 - \\text{edge}_0}, 0, 1\\right)$$\n",
    "\n",
    "The smoothstep function provides:\n",
    "- Smooth, continuous transitions (C¹ continuous)\n",
    "- Zero slope at the boundaries (no discontinuities)\n",
    "- Perceptually pleasing S-curve shape\n",
    "- Efficient computation (polynomial, no transcendental functions)\n",
    "\n",
    "### Algorithm 7.2: Whites Transform\n",
    "\n",
    "**What it does:** Adjusts bright tones (highlights) with smooth falloff toward midtones, preserving shadow detail.\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, whites adjustment value w ∈ ℝ\n",
    "Output: transformed pixel r'\n",
    "\n",
    "1. L ← (0.299·R + 0.587·G + 0.114·B) / 255    // Normalized luminance\n",
    "2. weight ← smoothstep(0.4, 0.8, L)            // Weight: high for bright, low for dark\n",
    "3. adjustment ← w × weight\n",
    "4. R' ← clamp(R + adjustment, 0, 255)\n",
    "5. G' ← clamp(G + adjustment, 0, 255)\n",
    "6. B' ← clamp(B + adjustment, 0, 255)\n",
    "7. return [R', G', B']ᵀ\n",
    "```\n",
    "\n",
    "**Key properties:**\n",
    "- Pixels with L < 0.4 get minimal adjustment (weight ≈ 0)\n",
    "- Pixels with L > 0.8 get full adjustment (weight ≈ 1)\n",
    "- Pixels in the 0.4-0.8 range get smooth transition\n",
    "- The adjustment is applied uniformly to all channels, preserving color relationships\n",
    "\n",
    "**Implementation:** See `applyWhites()` at lines 266-279 in `ImageCanvas.tsx`:\n",
    "\n",
    "```typescript\n",
    "const applyWhites = (rgb: RGB, value: number): RGB => {\n",
    "  if (value === 0) return rgb;\n",
    "  const luminance = (0.299 * rgb.r + 0.587 * rgb.g + 0.114 * rgb.b) / 255;\n",
    "  const weight = smoothstep(0.4, 0.8, luminance);\n",
    "  const adjustment = value * weight;\n",
    "  return {\n",
    "    r: clamp(rgb.r + adjustment),\n",
    "    g: clamp(rgb.g + adjustment),\n",
    "    b: clamp(rgb.b + adjustment)\n",
    "  };\n",
    "};\n",
    "```\n",
    "\n",
    "### Algorithm 7.3: Blacks Transform\n",
    "\n",
    "**What it does:** Adjusts dark tones (shadows) with smooth falloff toward midtones, preserving highlight detail.\n",
    "\n",
    "```\n",
    "Input: pixel r = [R, G, B]ᵀ, blacks adjustment value b ∈ ℝ\n",
    "Output: transformed pixel r'\n",
    "\n",
    "1. L ← (0.299·R + 0.587·G + 0.114·B) / 255    // Normalized luminance\n",
    "2. weight ← smoothstep(0.8, 0.2, L)            // Inverted: high for dark, low for bright\n",
    "3. adjustment ← b × weight\n",
    "4. R' ← clamp(R + adjustment, 0, 255)\n",
    "5. G' ← clamp(G + adjustment, 0, 255)\n",
    "6. B' ← clamp(B + adjustment, 0, 255)\n",
    "7. return [R', G', B']ᵀ\n",
    "```\n",
    "\n",
    "**Key properties:**\n",
    "- Pixels with L < 0.2 get full adjustment (weight ≈ 1)\n",
    "- Pixels with L > 0.8 get minimal adjustment (weight ≈ 0)\n",
    "- Pixels in the 0.2-0.8 range get smooth transition\n",
    "- The inverted smoothstep (edge₀ > edge₁) creates a curve that's high for dark pixels\n",
    "\n",
    "**Implementation:** See `applyBlacks()` at lines 281-294 in `ImageCanvas.tsx`:\n",
    "\n",
    "```typescript\n",
    "const applyBlacks = (rgb: RGB, value: number): RGB => {\n",
    "  if (value === 0) return rgb;\n",
    "  const luminance = (0.299 * rgb.r + 0.587 * rgb.g + 0.114 * rgb.b) / 255;\n",
    "  const weight = smoothstep(0.8, 0.2, luminance);\n",
    "  const adjustment = value * weight;\n",
    "  return {\n",
    "    r: clamp(rgb.r + adjustment),\n",
    "    g: clamp(rgb.g + adjustment),\n",
    "    b: clamp(rgb.b + adjustment)\n",
    "  };\n",
    "};\n",
    "```\n",
    "\n",
    "### Proof of Non-Composability\n",
    "\n",
    "**Claim:** Whites and blacks adjustments cannot be expressed as global affine transforms (matrix + offset).\n",
    "\n",
    "**Proof by contradiction:**\n",
    "\n",
    "Assume whites adjustment can be expressed as $W(\\mathbf{r}) = M\\mathbf{r} + \\mathbf{o}$ for some fixed $M$ and $\\mathbf{o}$.\n",
    "\n",
    "Consider two pixels with different luminances:\n",
    "- $\\mathbf{r}_1$ with $L_1 = 0.9$ (bright): weight $w_1 \\approx 1$, adjustment $\\approx v$\n",
    "- $\\mathbf{r}_2$ with $L_2 = 0.3$ (dark): weight $w_2 \\approx 0$, adjustment $\\approx 0$\n",
    "\n",
    "If whites were a linear transform, the adjustment would be constant for all pixels. But by construction, the adjustment depends on each pixel's luminance, which varies per pixel. Therefore, whites (and similarly blacks) cannot be global affine transforms and must be applied per-pixel.\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "1. **Smoothstep over linear interpolation:** Smoothstep provides perceptually smooth transitions with zero slope at boundaries, avoiding visible discontinuities that linear interpolation would create.\n",
    "\n",
    "2. **Luminance-based weighting:** Using Rec.601 luminance weights (0.299, 0.587, 0.114) ensures the adjustment respects perceived brightness, affecting pixels based on how bright they appear to human vision.\n",
    "\n",
    "3. **Uniform channel adjustment:** The adjustment is applied equally to R, G, B channels, preserving color relationships (hue and saturation) while only affecting brightness in the targeted tonal range.\n",
    "\n",
    "4. **Edge value selection:** \n",
    "   - Whites: edge₀=0.4, edge₁=0.8 targets the 40-80% luminance range, affecting highlights while preserving midtones and shadows\n",
    "   - Blacks: edge₀=0.8, edge₁=0.2 (inverted) targets the 20-80% luminance range, affecting shadows while preserving midtones and highlights\n",
    "\n",
    "5. **Per-pixel processing:** Like vibrance, whites and blacks are inherently per-pixel operations because the weight depends on each pixel's luminance value, making them non-composable with global affine transforms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8428b2",
   "metadata": {},
   "source": [
    "## 8. Transform Composition\n",
    "\n",
    "I realized that brightness, contrast, saturation, and hue are all affine transforms (matrix + offset). By composing them algebraically before processing pixels, I can apply multiple transforms with a single matrix multiplication per pixel.\n",
    "\n",
    "### Algorithm 8.1: Affine Transform Composition\n",
    "\n",
    "```\n",
    "Input: List of transforms T₁, T₂, ..., Tₙ where Tᵢ = (Mᵢ, oᵢ)\n",
    "Output: Composed transform T_composed = (M_composed, o_composed)\n",
    "\n",
    "1. M ← M₁\n",
    "2. o ← o₁\n",
    "3. for i = 2 to n:\n",
    "4.   M ← Mᵢ · M          // Matrix multiplication\n",
    "5.   o ← Mᵢ · o + oᵢ      // Transform offset through matrix\n",
    "6. return (M, o)\n",
    "```\n",
    "\n",
    "**Mathematical justification:** Composing two affine transforms:\n",
    "$$\\mathbf{r}'' = M_2(M_1\\mathbf{r} + \\mathbf{o}_1) + \\mathbf{o}_2 = (M_2 M_1)\\mathbf{r} + (M_2\\mathbf{o}_1 + \\mathbf{o}_2)$$\n",
    "\n",
    "So the composed transform is $(M_2 M_1, M_2\\mathbf{o}_1 + \\mathbf{o}_2)$.\n",
    "\n",
    "**Implementation:** See `composeAffineTransforms()` at lines 235-286 in `ImageCanvas.tsx`:\n",
    "```typescript\n",
    "const composeAffineTransforms = (\n",
    "  transforms: Array<{ matrix: number[]; offset: number[] }>\n",
    "): { matrix: number[]; offset: number[] } => {\n",
    "  if (transforms.length === 0) {\n",
    "    return { matrix: [1, 0, 0, 0, 1, 0, 0, 0, 1], offset: [0, 0, 0] };\n",
    "  }\n",
    "  \n",
    "  if (transforms.length === 1) {\n",
    "    return transforms[0];\n",
    "  }\n",
    "  \n",
    "  let resultMatrix = [...transforms[0].matrix];\n",
    "  let resultOffset = [...transforms[0].offset];\n",
    "  \n",
    "  for (let i = 1; i < transforms.length; i++) {\n",
    "    const M2 = transforms[i].matrix;\n",
    "    const o2 = transforms[i].offset;\n",
    "    \n",
    "    // Matrix multiplication: M_composed = M2 * M1\n",
    "    const newMatrix = [\n",
    "      M2[0] * resultMatrix[0] + M2[1] * resultMatrix[3] + M2[2] * resultMatrix[6],\n",
    "      M2[0] * resultMatrix[1] + M2[1] * resultMatrix[4] + M2[2] * resultMatrix[7],\n",
    "      M2[0] * resultMatrix[2] + M2[1] * resultMatrix[5] + M2[2] * resultMatrix[8],\n",
    "      M2[3] * resultMatrix[0] + M2[4] * resultMatrix[3] + M2[5] * resultMatrix[6],\n",
    "      M2[3] * resultMatrix[1] + M2[4] * resultMatrix[4] + M2[5] * resultMatrix[7],\n",
    "      M2[3] * resultMatrix[2] + M2[4] * resultMatrix[5] + M2[5] * resultMatrix[8],\n",
    "      M2[6] * resultMatrix[0] + M2[7] * resultMatrix[3] + M2[8] * resultMatrix[6],\n",
    "      M2[6] * resultMatrix[1] + M2[7] * resultMatrix[4] + M2[8] * resultMatrix[7],\n",
    "      M2[6] * resultMatrix[2] + M2[7] * resultMatrix[5] + M2[8] * resultMatrix[8]\n",
    "    ];\n",
    "    \n",
    "    // Offset transformation: o_composed = M2 * o1 + o2\n",
    "    const newOffset = [\n",
    "      M2[0] * resultOffset[0] + M2[1] * resultOffset[1] + M2[2] * resultOffset[2] + o2[0],\n",
    "      M2[3] * resultOffset[0] + M2[4] * resultOffset[1] + M2[5] * resultOffset[2] + o2[1],\n",
    "      M2[6] * resultOffset[0] + M2[7] * resultOffset[1] + M2[8] * resultOffset[2] + o2[2]\n",
    "    ];\n",
    "    \n",
    "    resultMatrix = newMatrix;\n",
    "    resultOffset = newOffset;\n",
    "  }\n",
    "  \n",
    "  return { matrix: resultMatrix, offset: resultOffset };\n",
    "};\n",
    "```\n",
    "\n",
    "\n",
    "### Proof of Correctness\n",
    "\n",
    "For transforms $T_1, T_2, \\ldots, T_n$, Algorithm 8.1 produces a transform $T$ such that:\n",
    "$$T(\\mathbf{r}) = T_n(T_{n-1}(\\cdots T_2(T_1(\\mathbf{r})) \\cdots))$$\n",
    "\n",
    "**Proof by induction:**\n",
    "\n",
    "*Base case ($n=1$):* Trivial—the single transform is returned as-is.\n",
    "\n",
    "*Inductive step:* Assume correctness for $n-1$ transforms. Let $(M', \\mathbf{o}')$ be the composition of $T_1, \\ldots, T_{n-1}$. After iteration $n$:\n",
    "$$M = M_n M', \\quad \\mathbf{o} = M_n\\mathbf{o}' + \\mathbf{o}_n$$\n",
    "\n",
    "Applying this to pixel $\\mathbf{r}$:\n",
    "$$T(\\mathbf{r}) = M\\mathbf{r} + \\mathbf{o} = M_n M'\\mathbf{r} + M_n\\mathbf{o}' + \\mathbf{o}_n$$\n",
    "$$= M_n(M'\\mathbf{r} + \\mathbf{o}') + \\mathbf{o}_n = T_n(T'(\\mathbf{r}))$$\n",
    "\n",
    "where $T'$ is the composition of $T_1, \\ldots, T_{n-1}$. By the inductive hypothesis, this equals the full composition. ∎\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ea84e",
   "metadata": {},
   "source": [
    "## 9. Processing Pipeline\n",
    "\n",
    "I scan through the transform order and batch all consecutive affine transforms, then apply per-pixel transforms individually when encountered. Convolution operations are applied separately as they operate on spatial neighborhoods rather than individual pixels.\n",
    "\n",
    "### Algorithm 9.1: Smart Batching Pipeline\n",
    "\n",
    "```\n",
    "Input: Pixel array P (as ImageData), ordered transform list L\n",
    "Output: Transformed pixel array P'\n",
    "\n",
    "1. i ← 0\n",
    "2. while i < length(L):\n",
    "3.   batch ← []\n",
    "4.   while i < length(L) and L[i] is affine:  // Collect affine transforms\n",
    "5.     batch.append(L[i])\n",
    "6.     i ← i + 1\n",
    "7.   if batch is not empty:\n",
    "8.     (M, o) ← composeAffineTransforms(batch)\n",
    "9.     for each pixel p in P:\n",
    "10.      p ← M · p + o\n",
    "11.   if i < length(L) and L[i] is per-pixel affine:    // Apply per-pixel transform\n",
    "12.     for each pixel p in P:\n",
    "13.       p ← L[i].apply(p)\n",
    "14.     i ← i + 1\n",
    "15.   if i < length(L) and L[i] is convolution:    // Apply convolution operation\n",
    "16.     P ← L[i].convolve(P)  // Operates on entire ImageData\n",
    "17.     i ← i + 1\n",
    "18. return P\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Affine transforms (brightness, contrast, saturation, hue) can be batched because they apply the same matrix to every pixel\n",
    "- Per-pixel transforms (vibrance, whites, blacks) adapt per pixel, so they must be applied individually\n",
    "- Convolution operations (blur, sharpen, edge, denoise, customConv) operate on spatial neighborhoods and must process the entire ImageData structure to access neighboring pixels\n",
    "- By batching consecutive affine transforms, I minimize per-pixel operations\n",
    "\n",
    "**Implementation:** See `ImageCanvas.tsx` lines 604-651. The pipeline processes transforms in order:\n",
    "\n",
    "```typescript\n",
    "for (const inst of pipeline) {\n",
    "  if (!inst.enabled) continue;\n",
    "  if (inst.kind === 'brightness' || inst.kind === 'contrast' || \n",
    "      inst.kind === 'saturation' || inst.kind === 'hue' || \n",
    "      inst.kind === 'vibrance') {\n",
    "    // Affine transforms: batch and compose\n",
    "    const batch: Array<{ matrix: number[]; offset: number[] }> = [];\n",
    "    if (kind === 'brightness') batch.push(buildBrightnessMatrix(...));\n",
    "    if (kind === 'contrast') batch.push(buildContrastMatrix(...));\n",
    "    // ... compose and apply ...\n",
    "  } else if (inst.kind === 'blur') {\n",
    "    const out = cpuConvolutionBackend.blur(imageData, p);\n",
    "    for (let j = 0; j < data.length; j++) data[j] = out.data[j];\n",
    "  } else if (inst.kind === 'sharpen') {\n",
    "    const out = cpuConvolutionBackend.sharpen(imageData, p);\n",
    "    // ... apply result ...\n",
    "  }\n",
    "  // ... other convolution operations ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Design Decision:** Convolutions are applied to the entire `ImageData` object rather than pixel-by-pixel because:\n",
    "1. They require access to neighboring pixels, which is naturally handled by the `ImageData` structure\n",
    "2. The convolution backend (`cpuConvolutionBackend`) operates on complete images, enabling optimizations like stride and efficient boundary handling\n",
    "3. This separation keeps the convolution logic modular and allows for future GPU acceleration\n",
    "\n",
    "**Complexity analysis:**\n",
    "Let $n$ be the number of transforms, $p$ be the number of pixels, and $k$ be the average kernel size for convolutions.\n",
    "\n",
    "Without batching: $O(np)$ matrix-vector multiplications + $O(nkp)$ convolution operations\n",
    "\n",
    "With batching: $O(n^2)$ matrix-matrix multiplications (negligible for small $n$) + $O(bp)$ matrix-vector multiplications (where $b$ is the number of batches) + $O(nkp)$ convolution operations\n",
    "\n",
    "For typical image processing where $n \\ll \\sqrt{p}$ (e.g., 5 transforms on a 2-megapixel image), batching provides significant performance gains for affine transforms. Convolutions remain $O(kp)$ per operation regardless of batching, as they are inherently spatial operations.\n",
    "\n",
    "**Order Dependency:**\n",
    "\n",
    "The pipeline applies transforms in the order specified by the user. This order matters because:\n",
    "- Affine transforms are linear and commute: $T_1(T_2(\\mathbf{r})) = T_2(T_1(\\mathbf{r}))$ for affine $T_1, T_2$\n",
    "- However, convolutions do not commute with affine transforms or other convolutions in general\n",
    "- For example, blurring then sharpening produces different results than sharpening then blurring\n",
    "\n",
    "Therefore, the pipeline respects user-specified order and applies each transform sequentially to the current image state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ceed0",
   "metadata": {},
   "source": [
    "## 10. Convolution Operations\n",
    "\n",
    "**What I wanted:** A framework for applying spatial filters to images through discrete convolution, enabling blur, sharpening, edge detection, and denoising operations.\n",
    "\n",
    "Unlike the affine color transforms (brightness, contrast, saturation, hue), convolutions operate on local neighborhoods of pixels. Each output pixel depends on a weighted sum of its surrounding pixels, making these operations inherently spatial and non-composable with global affine transforms.\n",
    "\n",
    "### 9.1 Mathematical Foundation\n",
    "\n",
    "**Discrete 2D Convolution:**\n",
    "\n",
    "For an image $I(x, y)$ and kernel $K(i, j)$ of size $n \\times n$ (typically odd), the convolution at pixel $(x, y)$ is:\n",
    "\n",
    "$$(I * K)(x, y) = \\sum_{i=-h}^{h} \\sum_{j=-h}^{h} I(x + i, y + j) \\cdot K(h + i, h + j)$$\n",
    "\n",
    "where $h = \\lfloor n/2 \\rfloor$ is the kernel half-size, and $K$ is centered at $(h, h)$.\n",
    "\n",
    "**Boundary Handling:**\n",
    "\n",
    "Since convolution requires accessing pixels outside image boundaries, we implement three padding modes:\n",
    "\n",
    "1. **Zero padding:** $I(x, y) = 0$ for out-of-bounds coordinates\n",
    "2. **Edge padding:** $I(x, y) = I(\\text{clamp}(x, 0, W-1), \\text{clamp}(y, 0, H-1))$\n",
    "3. **Reflect padding:** $I(x, y) = I(\\text{reflect}(x, W), \\text{reflect}(y, H))$ where reflection mirrors coordinates across boundaries\n",
    "\n",
    "**Implementation:** See `padIndex()` at lines 15-26 in `convolution.ts`:\n",
    "```typescript\n",
    "function padIndex(i: number, limit: number, mode: PaddingMode): number {\n",
    "  if (i >= 0 && i < limit) return i;\n",
    "  if (mode === 'zero') return -1; // sentinel for zero\n",
    "  if (mode === 'edge') return i < 0 ? 0 : limit - 1;\n",
    "  // reflect\n",
    "  let idx = i;\n",
    "  if (idx < 0) idx = -idx - 1;\n",
    "  const period = (limit - 1) * 2;\n",
    "  idx = idx % period;\n",
    "  if (idx >= limit) idx = period - idx;\n",
    "  return idx;\n",
    "}\n",
    "```\n",
    "\n",
    "### 9.2 Per-Pixel Convolution\n",
    "\n",
    "**Algorithm 10.1: Convolve at Single Pixel**\n",
    "\n",
    "```\n",
    "Input: source ImageData, pixel coordinates (x, y), kernel K[n×n], params\n",
    "Output: transformed RGB [R', G', B']ᵀ\n",
    "\n",
    "1. h ← ⌊n/2⌋\n",
    "2. rAcc ← 0, gAcc ← 0, bAcc ← 0\n",
    "3. for ky = 0 to n-1:\n",
    "4.   for kx = 0 to n-1:\n",
    "5.     ix ← x + (kx - h) · dilation\n",
    "6.     iy ← y + (ky - h) · dilation\n",
    "7.     (sx, sy) ← padIndex(ix, width, padding), padIndex(iy, height, padding)\n",
    "8.     if (sx, sy) is out-of-bounds (zero padding): continue\n",
    "9.     w ← K[ky][kx]\n",
    "10.    (R, G, B) ← source[sy · width + sx]\n",
    "11.    if perChannel:\n",
    "12.      rAcc ← rAcc + R · w\n",
    "13.      gAcc ← gAcc + G · w\n",
    "14.      bAcc ← bAcc + B · w\n",
    "15.    else:  // luminance-only\n",
    "16.      L ← 0.299·R + 0.587·G + 0.114·B\n",
    "17.      rAcc ← rAcc + L · w\n",
    "18.      gAcc ← gAcc + L · w\n",
    "19.      bAcc ← bAcc + L · w\n",
    "20. return [clamp(rAcc, 0, 255), clamp(gAcc, 0, 255), clamp(bAcc, 0, 255)]\n",
    "```\n",
    "\n",
    "**Implementation:** See `convolveAtPixel()` at lines 28-72 in `convolution.ts`:\n",
    "```typescript\n",
    "export function convolveAtPixel(\n",
    "  source: ImageData,\n",
    "  x: number,\n",
    "  y: number,\n",
    "  kernel: number[][],\n",
    "  params: ConvolutionCommonParams & { perChannel?: boolean }\n",
    "): [number, number, number] {\n",
    "  const { width, height, data } = source;\n",
    "  const k = kernel;\n",
    "  const kSize = k.length;\n",
    "  const kHalf = Math.floor(kSize / 2);\n",
    "  const dilation = params.dilation ?? 1;\n",
    "  const padding: PaddingMode = params.padding ?? 'edge';\n",
    "  const perChannel = params.perChannel ?? true;\n",
    "\n",
    "  let rAcc = 0, gAcc = 0, bAcc = 0;\n",
    "  for (let ky = 0; ky < kSize; ky++) {\n",
    "    for (let kx = 0; kx < kSize; kx++) {\n",
    "      const ix = x + (kx - kHalf) * dilation;\n",
    "      const iy = y + (ky - kHalf) * dilation;\n",
    "      let sx = padIndex(ix, width, padding);\n",
    "      let sy = padIndex(iy, height, padding);\n",
    "      const w = k[ky][kx];\n",
    "      if (sx === -1 || sy === -1) {\n",
    "        continue; // zero padding\n",
    "      }\n",
    "      const idx = (sy * width + sx) * 4;\n",
    "      const R = data[idx], G = data[idx + 1], B = data[idx + 2];\n",
    "      if (perChannel) {\n",
    "        rAcc += R * w;\n",
    "        gAcc += G * w;\n",
    "        bAcc += B * w;\n",
    "      } else {\n",
    "        const gray = 0.299 * R + 0.587 * G + 0.114 * B;\n",
    "        rAcc += gray * w;\n",
    "        gAcc += gray * w;\n",
    "        bAcc += gray * w;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return [clamp255(rAcc), clamp255(gAcc), clamp255(bAcc)];\n",
    "}\n",
    "```\n",
    "\n",
    "**Design Decision:** By default, we apply convolution per-channel (perChannel = true), treating R, G, B independently. This preserves color relationships better than luminance-only processing, which would desaturate edges and features. The luminance-only mode is available for specialized use cases.\n",
    "\n",
    "### 9.3 Blur Operations\n",
    "\n",
    "**Gaussian Blur:**\n",
    "\n",
    "Gaussian blur uses a kernel derived from the 2D Gaussian function:\n",
    "\n",
    "$$G(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "For a discrete $n \\times n$ kernel centered at $(0, 0)$:\n",
    "\n",
    "$$K[i][j] = \\exp\\left(-\\frac{(i-h)^2 + (j-h)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where $h = \\lfloor n/2 \\rfloor$. The kernel is normalized so $\\sum_{i,j} K[i][j] = 1$ to preserve image brightness.\n",
    "\n",
    "**Box Blur:**\n",
    "\n",
    "Box blur uses a uniform kernel:\n",
    "\n",
    "$$K[i][j] = \\frac{1}{n^2} \\quad \\forall i, j \\in [0, n-1]$$\n",
    "\n",
    "Box blur is computationally cheaper than Gaussian blur but produces less smooth results with visible artifacts.\n",
    "\n",
    "**Implementation:** See `gaussianKernel()` and `boxKernel()` at lines 112-136 in `convolution.ts`:\n",
    "```typescript\n",
    "export function gaussianKernel(size: 3 | 5 | 7, sigma?: number): number[][] {\n",
    "  const s = sigma ?? (size === 3 ? 0.85 : size === 5 ? 1.2 : 1.6);\n",
    "  const half = Math.floor(size / 2);\n",
    "  const k: number[][] = [];\n",
    "  let sum = 0;\n",
    "  for (let y = -half; y <= half; y++) {\n",
    "    const row: number[] = [];\n",
    "    for (let x = -half; x <= half; x++) {\n",
    "      const v = Math.exp(-(x * x + y * y) / (2 * s * s));\n",
    "      row.push(v);\n",
    "      sum += v;\n",
    "    }\n",
    "    k.push(row);\n",
    "  }\n",
    "  // normalize\n",
    "  for (let y = 0; y < size; y++) {\n",
    "    for (let x = 0; x < size; x++) k[y][x] /= sum;\n",
    "  }\n",
    "  return k;\n",
    "}\n",
    "```\n",
    "\n",
    "### 9.4 Sharpening Operations\n",
    "\n",
    "**Unsharp Masking:**\n",
    "\n",
    "Unsharp masking enhances edges by subtracting a blurred version from the original:\n",
    "\n",
    "$$\\text{sharpened} = \\text{original} + \\alpha(\\text{original} - \\text{blurred}) = (1+\\alpha)\\text{original} - \\alpha \\cdot \\text{blurred}$$\n",
    "\n",
    "This can be expressed as a single convolution kernel:\n",
    "\n",
    "$$K_{\\text{unsharp}} = (1+\\alpha)\\delta - \\alpha \\cdot K_{\\text{blur}}$$\n",
    "\n",
    "where $\\delta$ is the identity (delta) kernel (1 at center, 0 elsewhere).\n",
    "\n",
    "**Laplacian Sharpening:**\n",
    "\n",
    "The Laplacian operator detects edges by computing the second derivative. For sharpening:\n",
    "\n",
    "$$\\text{sharpened} = \\text{original} + \\alpha \\cdot \\text{Laplacian}(\\text{original})$$\n",
    "\n",
    "The discrete Laplacian kernel (3×3) is:\n",
    "\n",
    "$$K_{\\text{Laplacian}} = \\begin{bmatrix}\n",
    "0 & -\\alpha & 0 \\\\\n",
    "-\\alpha & 1+4\\alpha & -\\alpha \\\\\n",
    "0 & -\\alpha & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Implementation:** See `unsharpKernel()` and `laplacianKernel()` at lines 166-184 in `convolution.ts`:\n",
    "```typescript\n",
    "export function unsharpKernel(amount: number, size: 3 | 5): number[][] {\n",
    "  const blur = size === 3 ? boxKernel(3) : boxKernel(5);\n",
    "  const k = blur.map(row => row.map(v => -amount * v));\n",
    "  const c = Math.floor(size / 2);\n",
    "  k[c][c] += 1 + amount;\n",
    "  return k;\n",
    "}\n",
    "```\n",
    "\n",
    "### 9.5 Edge Detection\n",
    "\n",
    "**Gradient-Based Edge Detection:**\n",
    "\n",
    "Edge detection computes the image gradient $\\nabla I = [\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y}]^T$. The gradient magnitude indicates edge strength:\n",
    "\n",
    "$$|\\nabla I| = \\sqrt{\\left(\\frac{\\partial I}{\\partial x}\\right)^2 + \\left(\\frac{\\partial I}{\\partial y}\\right)^2}$$\n",
    "\n",
    "**Sobel Operator:**\n",
    "\n",
    "The Sobel operator approximates derivatives using:\n",
    "\n",
    "$$G_x = \\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "G_y = \\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The gradient components are computed via convolution:\n",
    "- $G_x = I * K_x$ (horizontal gradient)\n",
    "- $G_y = I * K_y$ (vertical gradient)\n",
    "\n",
    "The magnitude is: $|\\nabla I| = \\sqrt{G_x^2 + G_y^2}$\n",
    "\n",
    "**Prewitt Operator:**\n",
    "\n",
    "Similar to Sobel but with uniform weights:\n",
    "\n",
    "$$G_x = \\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "G_y = \\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Implementation:** See `applyEdge()` at lines 211-237 in `convolution.ts`:\n",
    "```typescript\n",
    "export function applyEdge(imageData: ImageData, params: EdgeParams): ImageData {\n",
    "  const { width, height, data } = imageData;\n",
    "  const out = new ImageData(width, height);\n",
    "  const { kx, ky } = params.operator === 'sobel' ? sobelKernels() : prewittKernels();\n",
    "  const perChannel = true;\n",
    "  const padding: PaddingMode = params.padding ?? 'edge';\n",
    "  for (let y = 0; y < height; y++) {\n",
    "    for (let x = 0; x < width; x++) {\n",
    "      const [rx, gx, bx] = convolveAtPixel(imageData, x, y, kx, { padding, perChannel });\n",
    "      const [ry, gy, by] = convolveAtPixel(imageData, x, y, ky, { padding, perChannel });\n",
    "      let r = 0, g = 0, b = 0;\n",
    "      if (params.combine === 'x') {\n",
    "        r = Math.abs(rx); g = Math.abs(gx); b = Math.abs(bx);\n",
    "      } else if (params.combine === 'y') {\n",
    "        r = Math.abs(ry); g = Math.abs(gy); b = Math.abs(by);\n",
    "      } else {\n",
    "        r = Math.hypot(rx, ry); g = Math.hypot(gx, gy); b = Math.hypot(bx, by);\n",
    "      }\n",
    "      const idx = (y * width + x) * 4;\n",
    "      out.data[idx] = clamp255(r);\n",
    "      out.data[idx + 1] = clamp255(g);\n",
    "      out.data[idx + 2] = clamp255(b);\n",
    "      out.data[idx + 3] = data[idx + 3];\n",
    "    }\n",
    "  }\n",
    "  return out;\n",
    "}\n",
    "```\n",
    "\n",
    "**Key insight:** Edge detection applies two separate convolutions (horizontal and vertical gradients), then combines them. This is fundamentally different from a single-kernel convolution and cannot be reduced to a single matrix operation.\n",
    "\n",
    "### 9.6 Denoising Operations\n",
    "\n",
    "**Mean Filter (Box Blur):**\n",
    "\n",
    "Mean filtering replaces each pixel with the average of its neighborhood, reducing noise but also blurring edges:\n",
    "\n",
    "$$\\text{denoised}(x, y) = \\frac{1}{n^2} \\sum_{i=-h}^{h} \\sum_{j=-h}^{h} I(x+i, y+j)$$\n",
    "\n",
    "We optionally blend the filtered result with the original:\n",
    "\n",
    "$$\\text{output} = (1-k) \\cdot \\text{original} + k \\cdot \\text{filtered}$$\n",
    "\n",
    "where $k \\in [0, 1]$ is the strength parameter.\n",
    "\n",
    "**Median Filter:**\n",
    "\n",
    "Median filtering replaces each pixel with the median of its neighborhood. Unlike mean filtering, median filtering preserves edges better while removing salt-and-pepper noise:\n",
    "\n",
    "$$\\text{denoised}(x, y) = \\text{median}\\{I(x+i, y+j) : i, j \\in [-h, h]\\}$$\n",
    "\n",
    "**Implementation:** See `applyDenoise()` at lines 239-293 in `convolution.ts`. The median filter implementation collects all pixels in the neighborhood, sorts them, and selects the middle value:\n",
    "```typescript\n",
    "// median filter\n",
    "const windowR = new Array<number>(kSize * kSize);\n",
    "const windowG = new Array<number>(kSize * kSize);\n",
    "const windowB = new Array<number>(kSize * kSize);\n",
    "// ... collect pixels ...\n",
    "windowR.sort((a, b) => a - b);\n",
    "windowG.sort((a, b) => a - b);\n",
    "windowB.sort((a, b) => a - b);\n",
    "const mid = Math.floor(windowR.length / 2);\n",
    "out.data[idxOut] = windowR[mid];\n",
    "```\n",
    "\n",
    "**Proof of Edge Preservation (Median Filter):**\n",
    "\n",
    "**Claim:** Median filtering preserves step edges better than mean filtering.\n",
    "\n",
    "**Proof:** Consider a step edge where pixels transition from value $a$ to value $b$ with $a < b$. \n",
    "\n",
    "For mean filtering: If the kernel straddles the edge, the output is a weighted average of $a$ and $b$, producing an intermediate value that blurs the edge.\n",
    "\n",
    "For median filtering: If more than half the kernel pixels are on one side of the edge, the median will be either $a$ or $b$ (depending on which side dominates), preserving the sharp transition. Only when the kernel is exactly centered on the edge does the median produce an intermediate value.\n",
    "\n",
    "Therefore, median filtering preserves edges more effectively than mean filtering. ∎\n",
    "\n",
    "### 9.7 Custom Convolution\n",
    "\n",
    "Custom convolution allows users to define arbitrary kernels, enabling experimentation with novel filters. The implementation applies the user-provided kernel directly via `convolveImageData()`.\n",
    "\n",
    "**Implementation:** See `applyCustomConv()` at lines 295-301 in `convolution.ts`:\n",
    "```typescript\n",
    "export function applyCustomConv(imageData: ImageData, params: CustomConvParams): ImageData {\n",
    "  return convolveImageData(imageData, params.kernel, { \n",
    "    stride: params.stride ?? 1, \n",
    "    padding: params.padding ?? 'edge', \n",
    "    perChannel: true \n",
    "  });\n",
    "}\n",
    "```\n",
    "\n",
    "### 9.8 Stride and Dilation\n",
    "\n",
    "**Stride:**\n",
    "\n",
    "Stride controls the spacing between output pixels. With stride $s$, we compute convolution only at positions $(x, y)$ where $x \\equiv 0 \\pmod{s}$ and $y \\equiv 0 \\pmod{s}$. For stride $> 1$, we fill skipped pixels using nearest-neighbor interpolation to avoid black gaps.\n",
    "\n",
    "**Dilation:**\n",
    "\n",
    "Dilation spaces out kernel elements. With dilation $d$, kernel element $K[i][j]$ is applied at offset $(i \\cdot d, j \\cdot d)$ instead of $(i, j)$. This effectively enlarges the kernel's receptive field without increasing its size.\n",
    "\n",
    "**Implementation:** See `convolveImageData()` at lines 74-110 in `convolution.ts` for stride handling, and `convolveAtPixel()` line 47 for dilation.\n",
    "\n",
    "### 9.9 Non-Composability with Affine Transforms\n",
    "\n",
    "**Claim:** Convolution operations cannot be expressed as global affine transforms (matrix + offset).\n",
    "\n",
    "**Proof by contradiction:**\n",
    "\n",
    "Assume a convolution operation $C$ can be expressed as $C(\\mathbf{r}) = M\\mathbf{r} + \\mathbf{o}$ for some fixed $3 \\times 3$ matrix $M$ and offset $\\mathbf{o}$.\n",
    "\n",
    "Consider two pixels $\\mathbf{r}_1$ and $\\mathbf{r}_2$ that are identical in value but have different spatial contexts (different neighbors). Under a convolution, these pixels will produce different outputs because convolution depends on local neighborhoods. However, under an affine transform:\n",
    "\n",
    "$$C(\\mathbf{r}_1) = M\\mathbf{r}_1 + \\mathbf{o} = M\\mathbf{r}_2 + \\mathbf{o} = C(\\mathbf{r}_2)$$\n",
    "\n",
    "This contradicts the spatial dependency of convolution. Therefore, convolutions cannot be global affine transforms and must be applied separately in the processing pipeline. \n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0641596",
   "metadata": {},
   "source": [
    "Bezryadin, S., Bourov, P., & Ilinih, D. (2007, January). Brightness calculation in digital image processing. In International symposium on technologies for digital photo fulfillment (Vol. 1, pp. 10-15). Society for Imaging Science and Technology. https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/tdpf/1/1/art00005\n",
    "\n",
    "Convolution. Wikipedia. https://en.wikipedia.org/wiki/Convolution\n",
    "\n",
    "Czech Technical University in Prague. (n.d.). Image Processing Fundamentals - Convolution-based Operations. https://cmp.felk.cvut.cz/cmp/courses/dzo/resources/course_ip_tudelft/course/fip-Convolut-2.html\n",
    "\n",
    "Discrete Laplace operator. Wikipedia. https://en.wikipedia.org/wiki/Discrete_Laplace_operator\n",
    "\n",
    "Huamán, A. (n.d.). Changing the contrast and brightness of an image!. OpenCV. https://docs.opencv.org/4.x/d3/dc1/tutorial_basic_linear_transform.html\n",
    "\n",
    "Intel Corporation. (2022). Two-Dimensional Finite Linear Convolution. Intel® Integrated Performance Primitives Developer Guide. https://www.intel.com/content/www/us/en/docs/ipp/developer-guide-reference/2022-1/convolution.html\n",
    "\n",
    "Kernel (image processing). Wikipedia. https://en.wikipedia.org/wiki/Kernel_%28image_processing%29\n",
    "\n",
    "Lisitsa, N. (2024, October 10). Transforming colors with matrices. lisyarus blog. https://lisyarus.github.io/blog/posts/transforming-colors-with-matrices.html\n",
    "\n",
    "McNair, M. (n.d.). Color Correction. https://maximmcnair.com/p/webgl-color-correction\n",
    "\n",
    "Meyer, M. (2009, January 19). Analyzing photoshop vibrance and saturation. Alaska Photographer Mark Meyer. https://www.photo-mark.com/notes/analyzing-photoshop-vibrance-and-saturation/\n",
    "\n",
    "Multidimensional discrete convolution. Wikipedia. https://en.wikipedia.org/wiki/Multidimensional_discrete_convolution\n",
    "\n",
    "NVIDIA Corporation. (n.d.). Convolution. NVIDIA Developer Documentation. https://developer.nvidia.com/discover/convolution\n",
    "\n",
    "Prewitt operator. Wikipedia. https://en.wikipedia.org/wiki/Prewitt_operator\n",
    "\n",
    "Relative luminance. Wikipedia. https://en.wikipedia.org/wiki/Relative_luminance\n",
    "\n",
    "Rodrigues’ rotation formula. Wikipedia. https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula\n",
    "\n",
    "Separable filter. Wikipedia. https://en.wikipedia.org/wiki/Separable_filter\n",
    "\n",
    "Sobel operator. Wikipedia. https://en.wikipedia.org/wiki/Sobel_operator\n",
    "\n",
    "sRGB. Wikipedia. https://en.wikipedia.org/wiki/SRGB\n",
    "\n",
    "University of Edinburgh. (n.d.). Convolution. HIPR2 - Hypermedia Image Processing Reference. https://homepages.inf.ed.ac.uk/rbf/HIPR2/convolve.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f37dfd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0169d3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27783c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a31ace57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bcdd54b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eed68f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6231af13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f50fff86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6735ef07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc65572d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d705bdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05e4027",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c751bd12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec35b578",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cee3194f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc79258",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e64e63f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8619cda3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790c83b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25bcd544",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bc2af5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba44365c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
